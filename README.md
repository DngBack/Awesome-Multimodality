# <p align=center>`Awesome Multimodality`</p>

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A collection of resources on multimodal learning research.

## <span id="head-content"> *Content* </span>
* - [ ] [1. Description](#head1)

* - [ ] [2. Paper With Code](#head2)
  * - [ ] [Survey](#head-Survey)
  * - [ ] [2021](#head-2021)
  * - [ ] [2020](#head-2020)

* - [ ] [3. Courses](#head4)

* [*Contact Me*](#head3)

## <span id="head1"> *1.Description* </span>

>🐌 Markdown Format:
> * (Conference/Journal Year) [Task/Keywords] **Title**, First Author et al. [[Paper](URL)] [[Code](URL)] [[Project](URL)]
> 
## <span id="head2"> *2.Paper With Code* </span>

* <span id="head-Survey"> **Survey**  </span>
    * (arXiv preprint 2021) **A Survey on Multi-modal Summarization**, Anubhav Jangra et al. [[v1](https://arxiv.org/pdf/2109.05199.pdf)](2021.09.11) 
* <span id="head-2021"> **2021**  </span>
    * (ICCV 2021 **Oral**) [Text-guided Image Manipulation] **StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery**, Or Patashnik et al. [[Paper](https://arxiv.org/abs/2103.17249)] [[Code](https://github.com/orpatashnik/StyleCLIP)] [[Play](https://replicate.ai/orpatashnik/styleclip)]
    * (ICCV 2021) [Facial Editing] **Talk-to-Edit: Fine-Grained Facial Editing via Dialog**, Yuming Jiang et al. [[Paper](https://arxiv.org/abs/2109.04425)] [[Code](https://github.com/yumingj/Talk-to-Edit)] [[Project](https://www.mmlab-ntu.com/project/talkedit/)] [[Dataset Project](https://mmlab.ie.cuhk.edu.hk/projects/CelebA/CelebA_Dialog.html)] [[Dataset(CelebA-Dialog Dataset)](https://drive.google.com/drive/folders/18nejI_hrwNzWyoF6SW8bL27EYnM4STAs)] 
    * (arXiv preprint 2021) [Video Action Recognition] **ActionCLIP: A New Paradigm for Video Action Recognition**, Mengmeng Wang et al. [[Paper](https://arxiv.org/abs/2109.08472)] 

* <span id="head-2020"> **2020**  </span>

## <span id="head4"> *3.Courses* </span>

* [CMU Multimodal Learning](https://cmu-multicomp-lab.github.io/mmml-course/fall2020/)

## <span id="head3"> *Contact Me* </span>

* [Yutong ZHOU](https://github.com/Yutong-Zhou-cv) in [Interaction Laboratory, Ritsumeikan University.](https://github.com/Rits-Interaction-Laboratory) ଘ(੭*ˊᵕˋ)੭

* If you have any question, please feel free to contact Yutong ZHOU (E-mail: <zhou@i.ci.ritsumei.ac.jp>).
